# Environment variables can be used anywhere in the config using ${VAR} or $VAR syntax.
# Example: ${MY_SECRET} or ${DATA_DIR:-/default/path} (with default value).
#
# Multiple config files can be merged with: --config base.yaml --config overrides.yaml
# Later files override earlier ones.

global:
  log_level: ${LOG_LEVEL:-info}
  client_logs_to_stdout: true
  docker_network: benchmarkoor
  cleanup_on_start: false
  # Optional directory configurations.
  # directories:
  #   # Directory for temporary datadir copies (defaults to system temp).
  #   tmp_datadir: ${TMP_DIR:-/tmp}/benchmarkoor
  #   # Directory for executor cache - git clones, etc (defaults to ~/.cache/benchmarkoor).
  #   tmp_cachedir: ${TMP_DIR:-/tmp}/benchmarkoor-cache
  # Optional: Override path to drop_caches file (default: /proc/sys/vm/drop_caches).
  # Useful when running in containers where the file is mounted at a different path.
  # drop_caches_path: /proc/sys/vm/drop_caches
  # Optional: GitHub token for downloading GitHub Actions artifacts via the REST API.
  # If the gh CLI is installed and authenticated, no token is needed.
  # Otherwise, provide a GitHub token with actions:read scope.
  # Can also be set via BENCHMARKOOR_GLOBAL_GITHUB_TOKEN environment variable.
  # github_token: ${GITHUB_TOKEN:-}

benchmark:
  results_dir: ${RESULTS_DIR:-./results}
  # Optional: Set ownership (user:group) for results files. Useful when running as root.
  # results_owner: "1000:1000"
  # Optional: Enable/disable system resource collection (cgroups/Docker Stats API).
  # When disabled, no CPU/memory/disk metrics will be collected during tests.
  # Useful when running in environments without cgroup access. Default: true
  # system_resource_collection_enabled: true
  # Optional: Generate index.json after benchmark (aggregates all run metadata).
  # generate_results_index: true
  # Optional: Generate stats.json per suite after benchmark (aggregates test durations
  # across runs for the UI heatmap).
  # generate_suite_stats: true
  # Optional test execution configuration.
  # tests:
  #   # Optional filter to run only tests matching this pattern.
  #   filter: ""
  #   source:
  #     # Option 1: Local directory source.
  #     # local:
  #     #   base_dir: ./benchmark-tests
  #     #   # Optional: Pre-run steps executed before tests (glob patterns).
  #     #   pre_run_steps:
  #     #     - "warmup/*.txt"
  #     #   # Test step patterns (glob patterns for setup/test/cleanup phases).
  #     #   steps:
  #     #     setup:
  #     #       - "tests/setup/*.txt"
  #     #     test:
  #     #       - "tests/test/*.txt"
  #     #     cleanup:
  #     #       - "tests/cleanup/*.txt"
  #
  #     # Option 2: Git repository source.
  #     # git:
  #     #   repo: https://github.com/example/gas-benchmarks.git
  #     #   version: main  # branch, tag, or commit hash
  #     #   # Optional: Pre-run steps executed before tests (glob patterns).
  #     #   pre_run_steps:
  #     #     - "funding/*.txt"
  #     #   # Test step patterns (glob patterns for setup/test/cleanup phases).
  #     #   steps:
  #     #     setup:
  #     #       - "tests/setup/*.txt"
  #     #     test:
  #     #       - "tests/test/*.txt"
  #     #     cleanup:
  #     #       - "tests/cleanup/*.txt"
  #
  #     # Option 3: EEST (Ethereum Execution Spec Tests) fixtures from GitHub releases.
  #     # Downloads fixtures directly from ethereum/execution-spec-tests releases.
  #     # Genesis files are auto-resolved per client type from the release.
  #     # eest_fixtures:
  #     #   github_repo: ethereum/execution-spec-tests
  #     #   github_release: benchmark@v0.0.6
  #     #   # Optional: Override the subdirectory within fixtures tarball.
  #     #   # fixtures_subdir: fixtures/blockchain_tests_engine_x  # default
  #     #   # Optional: Override URLs for fixtures/genesis tarballs.
  #     #   # fixtures_url: https://example.com/fixtures_benchmark.tar.gz
  #     #   # genesis_url: https://example.com/benchmark_genesis.tar.gz
  #
  #     # Option 3b: EEST fixtures from GitHub Actions artifacts.
  #     # Alternative to releases - downloads from workflow run artifacts.
  #     # Uses the gh CLI if available, otherwise falls back to the GitHub REST API
  #     # (requires global.github_token or BENCHMARKOOR_GLOBAL_GITHUB_TOKEN).
  #     # eest_fixtures:
  #     #   github_repo: ethereum/execution-spec-tests
  #     #   fixtures_artifact_name: fixtures_benchmark_fast  # Required (instead of github_release)
  #     #   genesis_artifact_name: benchmark_genesis         # Optional, defaults to 'benchmark_genesis'
  #     #   # Optional: Specify a specific workflow run ID (uses latest if not specified)
  #     #   # fixtures_artifact_run_id: "12345678901"
  #     #   # genesis_artifact_run_id: "12345678901"
  #     #   # fixtures_subdir also works with artifacts

client:
  config:
    # JWT secret for Engine API authentication (optional, has a default).
    # Can be set via environment variable for security.
    # jwt: ${JWT_SECRET:-5a64f13bfb41a147711492237995b437433bcbec80a7eb2daae11132098d7bae}
    # Optional: Drop memory caches during benchmark execution (Linux only, requires root).
    # Values: "disabled" (default), "tests" (between tests), "steps" (between setup/test/cleanup steps)
    # drop_memory_caches: "disabled"
    # Optional: Rollback strategy to reset client state after each test.
    # Values: "none", "rpc-debug-setHead" (default, capture block number before test, rollback via debug_setHead after)
    # rollback_strategy: "rpc-debug-setHead"
    # Optional: Container resource limits (applied to all instances by default).
    # resource_limits:
    #   # CPU pinning - use ONE of the following:
    #   #   cpuset_count: N    - Pick N random CPUs (new random selection each run)
    #   #   cpuset: [0, 1, 2]  - Pin to specific CPUs
    #   cpuset_count: 4
    #   # Memory limit (supports units: b, k, m, g, e.g., "16g", "4096m")
    #   memory: "16g"
    #   # Disable swap for the container (sets memory-swap equal to memory and swappiness to 0)
    #   swap_disabled: true
    #   # Block I/O throttling (optional)
    #   blkio_config:
    #     # Limit device read bandwidth (supports units: b, k, m, g)
    #     device_read_bps:
    #       - path: /dev/sdb
    #         rate: '12mb'
    #     # Limit device read IOPS (integer)
    #     device_read_iops:
    #       - path: /dev/sdb
    #         rate: '120'
    #     # Limit device write bandwidth
    #     device_write_bps:
    #       - path: /dev/sdb
    #         rate: '1024k'
    #     # Limit device write IOPS
    #     device_write_iops:
    #       - path: /dev/sdb
    #         rate: '30'
    #   # CPU frequency management (Linux only, requires root and cpufreq subsystem)
    #   # These settings are applied to the CPUs specified by cpuset/cpuset_count,
    #   # or all online CPUs if neither is specified.
    #   # Fixed CPU frequency (supports: "2000MHz", "2.4GHz", "MAX")
    #   cpu_freq: "2000MHz"
    #   # Enable/disable turbo boost (Intel: no_turbo, AMD: boost)
    #   cpu_turboboost: false
    #   # CPU frequency governor (common: performance, powersave, schedutil)
    #   # Defaults to "performance" when cpu_freq is set
    #   cpu_freq_governor: performance
    # Genesis file URLs per client type
    genesis:
      besu: https://github.com/nethermindeth/gas-benchmarks/raw/refs/heads/main/scripts/genesisfiles/besu/zkevmgenesis.json
      erigon: https://github.com/nethermindeth/gas-benchmarks/raw/refs/heads/main/scripts/genesisfiles/geth/zkevmgenesis.json
      geth: https://github.com/nethermindeth/gas-benchmarks/raw/refs/heads/main/scripts/genesisfiles/geth/zkevmgenesis.json
      nethermind: https://github.com/nethermindeth/gas-benchmarks/raw/refs/heads/main/scripts/genesisfiles/nethermind/zkevmgenesis.json
      nimbus: https://github.com/nethermindeth/gas-benchmarks/raw/refs/heads/main/scripts/genesisfiles/geth/zkevmgenesis.json
      reth: https://github.com/nethermindeth/gas-benchmarks/raw/refs/heads/main/scripts/genesisfiles/geth/zkevmgenesis.json

  # Default images per client (used when 'image' is not specified):
  #   geth:       ethpandaops/geth:performance
  #   nethermind: ethpandaops/nethermind:performance
  #   besu:       ethpandaops/besu:performance
  #   erigon:     ethpandaops/erigon:performance
  #   nimbus:     statusim/nimbus-eth1:performance
  #   reth:       ethpandaops/reth:performance

  # Optional: Pre-populated data directories per client type.
  # When configured, the source directory is prepared and mounted into the container,
  # and the init container is skipped (data is already initialized).
  # datadirs:
  #   geth:
  #     source_dir: ./data/snapshots/geth
  #     # container_dir: /data  # defaults to client's data directory if omitted
  #     # Method for preparing the data directory:
  #     #   copy           - parallel Go copy (default, works everywhere, has progress)
  #     #   overlayfs      - uses Linux overlayfs for near-instant setup (requires root)
  #     #   fuse-overlayfs - uses fuse-overlayfs for near-instant setup (no root required)
  #     #                    Requires: apt install fuse-overlayfs (or equivalent)
  #     #                    Requires: user_allow_other in /etc/fuse.conf if Docker runs as root
  #     #                    WARNING: fuse-overlayfs is SLOW compared to overlayfs. You'll get
  #     #                             3x Worse results.
  #     #   zfs            - uses ZFS snapshots and clones for near-instant, copy-on-write setup
  #     #                    Requires: source_dir must be on a ZFS filesystem
  #     #                    Requires: root access OR ZFS delegations configured:
  #     #                      zfs allow -u <user> clone,create,destroy,mount,snapshot <dataset>
  #     #                    The dataset is auto-detected from the source_dir mount point.
  #     method: copy
  #   reth:
  #     source_dir: ./data/snapshots/reth
  #     # container_dir defaults to /var/lib/reth for reth
  #     method: fuse-overlayfs  # near-instant, no root required
  #   # Example: ZFS-backed data directory
  #   # nethermind:
  #   #   source_dir: /tank/benchmarkoor/nethermind-snapshot  # Must be on ZFS
  #   #   container_dir: /nethermind/data
  #   #   method: zfs  # near-instant via ZFS clones

  instances:
    - id: geth-latest
      client: geth
      # image: ${GETH_IMAGE:-ethpandaops/geth:performance}
      # pull_policy: always (default)
      # Optional overrides:
      # entrypoint: []
      # command: []
      # extra_args:  # Additional arguments appended to command
      #   - --verbosity=5
      # restart: never
      # environment:
      #   SOME_VAR: ${MY_ENV_VAR}
      # genesis: <override-url>
      # datadir:  # Instance-level datadir (overrides global datadirs)
      #   source_dir: ${DATA_SNAPSHOTS_DIR}/geth
      #   container_dir: /data
      # drop_memory_caches: "steps"  # Instance-level override (optional)
      # rollback_strategy: "rpc-debug-setHead"  # Instance-level override (optional)
      # resource_limits:  # Instance-level override (optional)
      #   cpuset_count: 2  # Override global with fewer CPUs
      #   memory: "8g"     # Override global memory limit

    # Example: running multiple clients
    # - id: nethermind-latest
    #   client: nethermind
    #   # image: ethpandaops/nethermind:performance (default)

    # - id: besu-latest
    #   client: besu
    #   # image: ethpandaops/besu:performance (default)

    # - id: reth-latest
    #   client: reth
    #   # image: ethpandaops/reth:performance (default)

    # - id: erigon-latest
    #   client: erigon
    #   # image: ethpandaops/erigon:performance (default)

    # - id: nimbus-latest
    #   client: nimbus
    #   # image: statusim/nimbus-eth1:performance (default)
